{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n",
    "\n",
    "Instead of waiting for the full episode to complete, we can break it up and learn the $Q$ function directly from actual experience from the environment. We thus learn by **bootstrapping**, that is, updating as we play our knowledge of the value function. For the prediction problem, our goal is, given $\\pi$, learn $q_\\pi$ online from experience.\n",
    "\n",
    "One way to do that is by incremental every-visit Monte Carlo: we update estimates of the value $Q(S_t, A_t)$ towards **actual** return $G_t$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>Incremental Monte Carlo Update </h4>\n",
    "<ul>\n",
    " $$\\begin{aligned}\n",
    " Q(S_t,A_t) &\\leftarrow& Q(S_t,A_t)+\\alpha(\\underbrace{G_t}-Q(S_t,A_t))\n",
    " \\end{aligned}\n",
    " $$\n",
    " </ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TD way to do it is to, instead, update estimate of the value $V(S_t)$ towards the **estimated** return (called the TD target):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>TD Update</h4>\n",
    "<ul>\n",
    " $$\\begin{aligned}\n",
    " Q(S_t,A_t) &\\leftarrow& Q(S_t,A_t) + \\alpha \\left (\\underbrace{R_{t+1}+\\gamma Q(S_{t+1}, A_{t+1})}_{\\text{=:TD target}}-Q(S_t, A_t) \\right )\n",
    " \\end{aligned}\n",
    " $$\n",
    " </ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these two methods compare? Well,\n",
    "- TD updates the \"first order\" approximation of the expected return, instead of the average return.\n",
    "- TD can learn before the final episode, unlike MC.\n",
    "- TD works in non-terminating environments, unlike MC.\n",
    "- MC is an **unbiased** estimate, whereas TD introduces bias because we update not to the real value function, but towards or best guess so far. \n",
    "- TD has less variance: return depends on only one noisy estimate (action), whereas MC has noise across all trajectory.\n",
    "- TD exploits the Markov property (hence more effective in Markov environments).\n",
    "- MC does not exploit the Markov property (more effective in non-Markov environments).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more dramatic example: Assume you are driving and you come to an intersection. You don't see a car coming towards you, and you stop. If you took decisions by MC, you would have to crash to see the reward (negative). With TD, you don't need to wait until you die to update your value function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two learning procedures for evaluating a policy (which you will later improve). \n",
    "\n",
    "- **On-policy learning** which means learning about $\\pi$ by sampling experience from $\\pi$. \n",
    "\n",
    "- **Off-policy learning** learns about *optimal* policy while following an *exploratory* policy. \n",
    "\n",
    "An example of on-policy algorithm is SARSA, shown below. It get its name from the sequence $S,A,R,S',A'$ used to generate the updates. At each step we obtain, from the current policy, an action $A$ to be used in state $S$, then that gives us a reward $R$, and move to the new state $S'$. Using the *same* policy we get the new action $A'$ and we update our $Q$ function doing a gradient step on the improvement direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4> Sarsa Algorithm </h4>\n",
    "Initialize $Q(s,a)$ arbitrarily. $\\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}, \\ Q(\\text{terminal state},\\cdot) = 0$. \n",
    "    <ul>\n",
    "        <li> Repeat for each episode: </li>\n",
    "        Initialize the initial state $S$.\n",
    "        <p>Repeat, for each step of the episode:\n",
    "        <ul>\n",
    "            \n",
    "            <li> Choose $A$ from $S$ using the policy derived from $Q$, for instance, using $\\epsilon-$greedy</li>\n",
    "            <li> Take action $A$, observe $R$, $S'$ </li>\n",
    "            <li> Choose action $A'$ from $S'$ </li>\n",
    "            <li> $Q(S,A) <= Q(S,A)+\\alpha \\left [ R+\\gamma\\cdot Q(S',A')-Q(S,A) \\right ] $</li>\n",
    "            <li> $S <=S'$ </li>\n",
    "        </ul>\n",
    "        \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning, the second algorithm shown below, is off-policy: instead of steering the $Q-$function estimate using another data point generated from our policy, we use our knowledge to move it to the direction of the best possible action on the new state $S'$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4> Q-Learning Algorithm </h4>\n",
    "Initialize $Q(s,a)$ arbitrarily. $\\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}, Q(\\text{terminal state},\\cdot) = 0$. \n",
    "    <ul>\n",
    "        <li> Repeat for each episode: </li>\n",
    "        Initialize the initial state $S$.\n",
    "        <p>Repeat, for each step of the episode:\n",
    "        <ul>\n",
    "            \n",
    "            <li> Choose $A$ from $S$ using the policy derived from $Q$, for instance, using $\\epsilon-$greedy.</li>\n",
    "            <li> Take action $A$, observe $R$, $S'$. </li>\n",
    "            <li> $Q(S,A) \\leftarrow Q(S,A)+\\alpha \\left [ R+\\gamma\\cdot \\max_{a \\in A} Q(S',a)-Q(S,A) \\right ] $</li>\n",
    "            <li> $S \\leftarrow S'$ </li>\n",
    "        </ul>\n",
    "        \n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see some sample code for implementing SARSA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def epsilon_greedy_policy(Q, epsilon, actions):\n",
    "    \"\"\" Q is a numpy array, epsilon between 0,1 \n",
    "    and a list of actions\"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        if np.random.rand()>epsilon:\n",
    "            action = np.argmax(Q[state,:])\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "gamma = 0.99 \n",
    "alpha = 0.1\n",
    "n_episodes = 10000\n",
    "\n",
    "\n",
    "actions = range(env.action_space.n)\n",
    "\n",
    "score = []    \n",
    "for j in range(n_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    policy = epsilon_greedy_policy(Q, \\\n",
    "                                   epsilon=10./(j+1), actions = actions )\n",
    "    \n",
    "    \n",
    "    ### Generate sample episode\n",
    "    t=0\n",
    "    while not done:\n",
    "        t+=1\n",
    "        action = policy(state)    \n",
    "        new_state, reward, done, _ =  env.step(action)\n",
    "        new_action = policy(new_state)\n",
    "        \n",
    "        #Book-keeping\n",
    "        if done:\n",
    "            Q[state, action] = Q[state,action] + alpha*(reward-Q[state,action])\n",
    "        else:\n",
    "            Q[state, action] = Q[state,action] + alpha*(reward+gamma*Q[new_state,new_action]-Q[state,action])\n",
    "            \n",
    "        state, action = new_state, new_action\n",
    "            \n",
    "        if done:\n",
    "            if len(score) < 100:\n",
    "                score.append(reward)\n",
    "            else:\n",
    "                score[j % 100] = reward\n",
    "                \n",
    "                \n",
    "            #if (j+1)%1000 == 0:\n",
    "                #print(\"INFO: Episode {} finished after \\\n",
    "                #{} timesteps with r={}.\\\n",
    "                # Running score: {}\".format(j+1, t, reward, np.mean(score)))\n",
    "            \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
