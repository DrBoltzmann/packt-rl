{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "There are two main ways of modelling reinforcement learning problems: **model-free** and **model-based**. Model-free methods do not even try to understand the environment: instead, they try to behave optimally there. In contrast, model-based methods require an approximation of the environment first, in order to behave optimally there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov chains\n",
    "\n",
    "The underlying theory is built on top of Markov chains. Markov chains are mathematical systems that describe situations where there are changes from one state to another. \n",
    "\n",
    "If you are not familiar, a great visual resource to read more about Markov chains is [http://setosa.io/ev/markov-chains/](http://setosa.io/ev/markov-chains/). \n",
    "\n",
    "\n",
    "The most important property of Markov chains is the **Markov property**:\n",
    "\n",
    "A sequence of random variables \n",
    "\n",
    "$$(S_t)_{t\\geq 0}$$ \n",
    "\n",
    "is a **Markov** chain if, for all $t$:\n",
    "\n",
    "$$ \\mathbb P(S_{t+1} \\ | \\ S_t ) = \\mathbb P(S_{t+1} \\ | \\ S_1, S_2, \\ldots S_t ) $$\n",
    "\n",
    "This means that the present is a sufficient statistic of the future: in other words, the current state has all the information we need.\n",
    "\n",
    "For two states $s,s'$ the **state transition probability** is defined by\n",
    "\n",
    "$$ p(s' \\ | \\ s) = \\mathbb{P} (S_{t+1} = s' \\ | \\ S_t = s). $$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>Example (Markov Chain)</h4>\n",
    "\n",
    "<p>\n",
    "Look at the figure below. Suppose we start on \"Wake up\". An **episode** is the path followed by the Markov chain. \n",
    "\n",
    "These are some sample episodes:\n",
    "<ul>\n",
    "    <li> Wake up, Facebook, Facebook, Study, Sleep </li>\n",
    "    <li> Wake up, Facebook, Facebook, Study, Hospoda, Sleep </li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "This is not a valid episode:\n",
    "<ul>\n",
    "    <li>Wake up, Facebook, Facebook, Study, Study, Sleep</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<img src = 'images/lec6_mchain.png' alt ='Example of a Markov chain' height=400 width=400>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the same process in the example above with the following **transition matrix**:\n",
    "\n",
    "  |   | Wake up | Facebook | Study | Hospoda | Sleep |\n",
    "| - | - |- |- |- |- |\n",
    "| Wake up | 0 |0.6 |0.4 |0 |0 \n",
    "| Facebook |0 |0.7 |0.1 |0.2 |0 \n",
    "| Study |0 |0 |0 |0.6 |0.4 \n",
    "| Hospoda |0 |0 |0 |0.3 |0.7 \n",
    "| Sleep |0 |0 |0 |0 | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "\n",
    "A **Markov reward process** is a tuple: \n",
    "\n",
    "$$\\left <\\mathcal S, p, \\mathcal R, \\gamma \\right >$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathcal S$ is a finite set of states.\n",
    "- $ p$ is a state transition probability.\n",
    "- $\\mathcal R$ is the set of rewards.\n",
    "- $\\gamma$ is a **discount factor**, $\\gamma \\in [0,1]$.\n",
    "\n",
    "This situation is illustrated in the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>Example (Markov Reward Process) </h4>\n",
    "An example is simply attaching rewards to any state of the previous Markov chain. Note that this still does not say anything about decision making: it is simply quantifying different trajectories when walking through the Markov chain.\n",
    "<img src = 'images/lec6_mrp.png' alt ='Example of a Markov reward process' height=400 width=400>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **return** $G_t$ is the total discounted reward from time step $t$:\n",
    "$$ G_t := R_{t+1}+\\gamma R_{t+2}+\\ldots = \\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}.$$\n",
    "\n",
    "\n",
    "The discount factor is the present value of future rewards. Introducing this factor is mathematically convenient (makes the infinite sum converge, i.e. have a finite value). However, it does have an interpretation: this represents the fact that immediate reward is more important than delayed reward, which is consistent with animal/human behaviour. A discount factor $\\gamma$ close to 0 is represents a myopic agent which cares only of instant gratification, whereas a discount factor of $\\gamma$ close to 1 represents an agent with far-sighted evaluation.\n",
    "\n",
    "      \n",
    "The **value function**, $v:\\mathcal S \\to \\mathbf R$ calculates, for every state $s \\in \\mathcal S$,  the expected return of all trajectories starting from $s$. That is, for all $t$,:\n",
    "\n",
    "$$v(s) := \\mathbb E( G_t \\ | \\ S_t =s )$$\n",
    "\n",
    "where $G_t$ is the return, i.e.\n",
    "$$ G_t =  R_{t+1}+\\gamma R_{t+2}+\\ldots = \\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}. $$ \n",
    "\n",
    "\n",
    "We can decompose the return in two parts: \n",
    "  * immediate reward $R_{t+1}$, and\n",
    "  * discounted value of the successor state, $S_{t+1}.$\n",
    "\n",
    "That is,\n",
    "\n",
    "$$v(s) := \\mathbb E( R_{t+1}+\\gamma \\cdot v(S_{t+1}) \\ | \\ S_t =s ).$$\n",
    "\n",
    "\n",
    "This is a linear equation, which we can solve directly:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v & = & \\mathcal R + \\gamma \\mathcal Pv \\\\\n",
    "(I-\\gamma\\mathcal P)v & = &  \\mathcal R \\\\\n",
    " v & = & (I-\\gamma\\mathcal P)^{-1}\\mathcal R\n",
    "\\end{aligned}$$\n",
    "\n",
    "The computational complexity is cubic in the number of states/actions, which means that direct solution is only possible for small MRPs. There are some iterative methods to help, as we'll see later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "Let us finally describe the model with full decision-making features. Assume the following situation:\n",
    "\n",
    "* Interactions happen in discrete time $t=1,2,3, \\ldots$\n",
    "* A finite set of **states**.\n",
    "* A finite set of **actions**.\n",
    "* A **reward function,** \n",
    "    $$\\mathcal R:\\mathcal S \\to \\mathbf R$$.\n",
    "* A transition function $p$ such that:\n",
    "\n",
    "\t$$\\mathbb P \\left(S_{t+1} = s' \\ | \\ S_t = s, A_t = a  \\right) = p(s', s, a).$$\n",
    "\n",
    "\n",
    "\n",
    "A **Markov decision process** is a tuple \n",
    "\n",
    "$$\\left (\\mathcal S, \\mathcal A, p, r,  \\gamma \\right )$$ \n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathcal S$ is a finite set of states.\n",
    "* $\\mathcal A$ is a finite set of actions.\n",
    "* $\\mathcal p(s' \\ | \\ s, a) = \\mathbb P (S_{t+1} = s \\ | \\ S_t = s, A_t = a).$\n",
    "* $r:\\mathcal S \\times \\mathcal A \\to \\mathbb R $ is a reward function \n",
    "* $\\gamma$ is a **discount factor**, $\\gamma \\in [0,1]$.\n",
    "\n",
    "\n",
    "A **policy** $\\pi$ is a distribution over actions given states, \n",
    "\n",
    "$$\\pi(a \\ | \\ s) := \\mathbb P(A_t = a \\ | \\ S_t = s).$$ \n",
    "\n",
    "* A policy fully defines the behaviour of the agent.\n",
    "* Policies are stationary and depend only on the current state, not the history.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>Example (Markov Decision Process)</h4>\n",
    "<img src = 'images/lec6_mdp.png' alt ='Example of a Markov Decision Process' height=400 width=400>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an MDP  $\\left (\\mathcal S, \\mathcal A,  p, r, \\gamma \\right )$ and a policy $\\pi$. The sequence of states $S_1, S_2, \\ldots$ is a Markov process with transition probabiliy matrix $\\mathcal P^\\pi$. \n",
    "\n",
    "The state and reward sequences $S_1, R_2, S_2, \\ldots$ is a Markov reward process $\\left (\\mathcal S, \\mathcal P^\\pi, \\mathcal R^\\pi, \\gamma \\right )$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p^\\pi (s' \\ | \\ s, \\pi) & := & \\sum_{a \\in \\mathcal A}\\pi(a \\ | \\ s)\\cdot p(s' \\ | \\ s,a)  \\\\\n",
    "r(s,\\pi )& = & \\sum_{a \\in \\mathcal A}\\pi(a \\ | \\ s)\\cdot \\mathcal r(s,a)\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "The **state-value function** $v_\\pi(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\\pi$:\n",
    "$$ v_\\pi(s) =\\mathbb E (G_t \\ | \\ S_t =s).$$ \n",
    "\n",
    "The **state-action value function** $q_\\pi(s,a)$ is the expected return starting from state $s$, taking action $a$ and then following policy $\\pi$, we get\n",
    "\n",
    "$$q_\\pi(s,a) = \\mathbb E_\\pi (G_t \\ | \\ S_t = s, A_t = a).$$\n",
    "\n",
    "\n",
    "As with MRP's, we have similar recursive relations:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "v_\\pi(s) & = & \\mathbb E_\\pi(R_{t+1}+\\gamma v_\\pi(S_{t+1}) \\ | \\ S_t = s) \\\\\n",
    "q_\\pi(s,a) & = & \\mathbb E_\\pi(R_{t+1}+\\gamma q_\\pi(S_{t+1},A_{t+1}) \\ | \\ S_t = s, A_t = a)\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we can, for a given policy $\\pi$, solve the Bellman equation.\n",
    "\n",
    "Note also that:\n",
    "\n",
    "$$v_\\pi(s) = \\sum_{a\\in \\mathcal A} \\pi(a \\ | \\ s)q_\\pi(s,a).$$\n",
    "\n",
    "\n",
    "The **optimal state-value function** $v_*(s)$ is the maximum over all policies:\n",
    "\n",
    "$$ v_*(s) = \\max_\\pi v_\\pi(s). $$\n",
    "\n",
    "The **optimal state-action-value function** $q_*(s,a)$ is the maximum over all policies:\n",
    "$$ q_*(s,a) = \\max_\\pi q_\\pi(s,a).$$\n",
    "\n",
    "This gives us a natural partial order over policies, \n",
    "\n",
    "$$\\pi \\geq \\pi' \\ \\Leftrightarrow \\ v_\\pi(s) \\geq v_{\\pi'}(s), \\ \\forall s \\in \\mathcal S. $$  \n",
    "\n",
    "The following theoretical result tells us that we are not doomed in our attempts to find optimal policies:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h4> Theorem </h4>\n",
    "\n",
    "There exists an optimal policy $\\pi_*$, that is better or equal than all other policies, $\\pi_* \\geq \\pi$. Moreover, all optimal policies achieve the optimal value functions $v_*$ and $q_*.$ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimal policy can be found maximizing over $q_*$:\n",
    "\n",
    "$$ \\pi_*(a,s) = 1 \\text{ if } a \\in \\mathrm{argmax}q_*(s,a) \\text{ else 0} .$$\n",
    "\n",
    "For Markov Decision Processes there is always a deterministic optimal policy (as opposed to two-player games, where there might not be optimal deterministic strategies).\n",
    "\n",
    "\n",
    "Some important properties derive from the recursion relations satisfied by the value functions. First, observe that\n",
    "\n",
    "$$v_*(s) = \\max_{a \\in \\mathcal A}q_*(s,a).$$\n",
    "\n",
    "Since \n",
    "\n",
    "$$q_*(s,a) = r(s,a) + \\gamma\\sum_{s' \\in \\mathcal S}p(s' \\ | \\ s,a)\\cdot v_*(s')$$\n",
    "\n",
    "(I'm playing the optimal policy, so in the next stage the payoff will be the optimal value).\n",
    "\n",
    "\n",
    "The following two recursive relations hold:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "v_*(s) & = & \\max_{a \\in \\mathcal A}\\left\\{ r(s,a) + \\gamma\\sum_{s' \\in \\mathcal S}\\mathcal p(s' \\ | \\ s,a)v_*(s')\\right \\} \\\\\n",
    "q_*(s,a) & = & r(s,a) + \\gamma\\sum_{s' \\in \\mathcal S}p(s' \\ | \\ s,a)\\cdot  \\max_{a' \\in \\mathcal A}q_*(s',a')\n",
    "\\end{aligned}$$\n",
    "\n",
    "This is called the **Bellman optimality equation**. The optimality equation is in general non-linear (as opposed to the MRP case) and has no closed form solution, in general. However, these relations are the basis for many solution methods.\n",
    "\n",
    "So how do I use the value functions to get an optimal policy? Knowing the optimal value functions, I know how good is each state => I know what is better to do. However, in general, $v_*$, $q_*$ are not known exactly, only approximated!\n",
    "\n",
    "The value functions are not exactly useful in many cases, since they depend on a precise description (model) of the problem. Although not realistic in many circumstances, we can use similar ideas and, perhaps the better argument in favor of MDPs, is that they provide the solid theoretical foundations for many model-free methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving MDPs: Value and Policy Iteration.\n",
    "\n",
    "If a representation of the model is available, that is, if we know the transition probabilities, we can use the theory of Markov Decision Processes to help us find optimal policies/value functions. \n",
    "The first method is called **value iteration**. The idea is to use the optimality equation:\n",
    "\n",
    "$$v_\\pi(s)  =  \\max_{a \\in \\mathcal A}\\left\\{ r(s,a) + \\gamma\\sum_{s' \\in \\mathcal S} p(s' \\ | \\ s,a)\\cdot v_*(s')\\right \\}$$\n",
    "\n",
    "If we denote by $\\mathbf T(f)$ the operator:\n",
    "\n",
    "$$\\mathbf T(f):=\\max_{a \\in \\mathcal A}\\left\\{ r(s,a) + \\gamma\\sum_{s' \\in \\mathcal S} p(s' \\ | \\ s,a)\\cdot f(s')\\right \\}$$\n",
    "\n",
    "then $\\mathbf T$ is a $\\gamma-$contraction, i.e. brings functions together:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|\\mathbf T (f) - \\mathbf T(g)\\|_\\infty \\leq \\|f-g\\|_\\infty\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $\\|f-g\\|_\\infty =\\max_{s\\in \\mathcal S}|f(s)-g(s)|$.\n",
    "\n",
    "If we use this operator to get a sequence:\n",
    "\n",
    "$$v_{k+1}  \\leftarrow \\max_{a \\in \\mathcal{A}} \\left \\{ \\mathcal{R}^a_s + \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{P}^a_{ss'}v_k(s')\\right\\}$$\n",
    "\n",
    "by the **contraction mapping theorem**, the sequence $(v_k)_{k\\geq 0}$ converts to a unique fixed point.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4> Value Iteration Algorithm </h4>\n",
    "<ul>\n",
    "<li>Given policy $\\pi:$ </li>\n",
    "    <ul>\n",
    "\t <li><b>Evaluate</b> $\\pi$, that is, calculate $v_\\pi(s)$.</li>\n",
    "     <li><b>Improve</b> the policy by being greedy with respect to $v_\\pi$. This means that the new policy $\\pi'$ should put more weight on actions that lead to better states with respect to $v_\\pi$.</li>\n",
    "\t<li> Update and repeat. </li>\n",
    "    to get a sequence:\n",
    "\n",
    "$$v_{k+1}  \\leftarrow \\max_{a \\in \\mathcal{A}} \\left \\{ r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}}p(s' \\ | \\ s,a)\\cdot v_k(s')\\right\\}$$\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also update the policies directly, via **policy iteration**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4> Policy Iteration Algorithm </h4>\n",
    "<ul>\n",
    "<li>Given policy $\\pi:$ </li>\n",
    "    <ul>\n",
    "\t <li><b>Evaluate</b> $\\pi$, that is, calculate $v_\\pi(s)$.</li>\n",
    "     <li><b>Improve</b> the policy by being greedy with respect to $v_\\pi$. This means that the new policy $\\pi'$ should put more weight on actions that lead to better states with respect to $v_\\pi$.</li>\n",
    "\t<li> Update and repeat. </li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this all works for a **known** MDP: if we have access to the internal mechanisms of the environment, then we can calculate this. However, very often we either have no idea (what is the optimal layout of a website?) or it's simply too complicated (what is the next likely screen in a video game?)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
