{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Policy optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RL as black-box optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of the tutorial let's implement CEM and NES for a simple test function (defined below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the function we want to optimize for sanity check\n",
    "def f(theta):\n",
    "  # Here would go the evaluation of the episode\n",
    "  reward = -np.sum(np.square(solution - theta))\n",
    "  return reward\n",
    "\n",
    "solution = np.array([1, 1, -0.5])\n",
    "\n",
    "# sanity check \n",
    "f(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# STARTER CODE - CEM\n",
    "#################################\n",
    "dim_theta = 3\n",
    "theta_mean = np.zeros(dim_theta)\n",
    "theta_std = np.ones(dim_theta)\n",
    "batch_size = 25 # number of samples per batch\n",
    "elite_frac = 0.2 # fraction of samples used as elite set\n",
    "\n",
    "\n",
    "n_iter = 300\n",
    "# Now, for the algorithms\n",
    "for _ in range(n_iter):\n",
    "\n",
    "    # Sample parameter vectors    \n",
    "\n",
    "    #YOUR CODE HERE    \n",
    "\n",
    "    # Get elite parameters\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    # Update theta_mean, theta_std\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    pass # remove/comment this line afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# STARTER CODE - NES\n",
    "#################################\n",
    "w0 = np.random.randn(3) #initial guess\n",
    "npop=50\n",
    "n_iter=300\n",
    "sigma=0.1\n",
    "alpha=0.001\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    # Sample vectors from a normal distribution\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Sample function values by evaluating on the population\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Optional: standardize (substract mean and divide by std)\n",
    "    \n",
    "    # \"Gradient\" update\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass # remove/comment this line afterwards\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "- How can you re-use your code above for `CartPole`/`FrozenLake`? \n",
    "- Try to solve one of these environments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Montecarlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example implementation of REINFORCE, as explained in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(s,a):\n",
    "    return (2*a-1)*np.tanh(s)\n",
    "\n",
    "def softmax_policy(theta, actions):\n",
    "    def policy_fn(state):\n",
    "        exp = np.exp([np.dot(featurize(state,a),theta) for a in actions])\n",
    "        probs = exp/np.sum(exp)\n",
    "        return probs\n",
    "    return policy_fn\n",
    "\n",
    "def run_episode(env, random_policy): \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    actions = range(env.action_space.n)\n",
    "    while not done:\n",
    "        probs = random_policy(state)\n",
    "        action = np.random.choice(actions, p=probs)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "    return episode, total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 2*np.random.rand(4)-1\n",
    "n_episodes = 10000\n",
    "actions = range(env.action_space.n)\n",
    "gamma = 1.\n",
    "alpha = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward in episode 1000: 99.0\n",
      "Won! this happened in episode  1244\n"
     ]
    }
   ],
   "source": [
    "for ep in range(n_episodes):\n",
    "    G = 0\n",
    "    policy = softmax_policy(theta,actions)\n",
    "    episode, ep_reward = run_episode(env,policy)\n",
    "    \n",
    "    if ep_reward == 200:\n",
    "        print(\"Won! this happened in episode \", ep)\n",
    "        break\n",
    "    \n",
    "    if (ep+1) % 1000 == 0:\n",
    "        print(\"Reward in episode {}: {}\".format(ep+1,ep_reward))\n",
    "    \n",
    "    \n",
    "    for i, _ in enumerate(episode):\n",
    "        s, a, r = _\n",
    "        G = sum(x[2]*(gamma**j) for j, x in enumerate(episode[i:]))\n",
    "        s = np.array(s)\n",
    "        theta = theta + alpha*(featurize(s,a)-np.sum([policy(s)[a]*featurize(s,a) for a in actions]))*G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn:\n",
    "\n",
    "- Implement REINFORCE and REINFORCE with baseline for `CliffWalking`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional material\n",
    "\n",
    "If you feel ready for a more interesting challenge, you can try `Pong`, which is one of the simples Atari environments. You should do some preprocessing of the video frames, to speed up the computation (resize, downsample, and remove background or color altogether).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to solve this yourself first! If you get stuck or want some inspiration, look at Andrej Karpathy's post:\n",
    "\n",
    "- http://karpathy.github.io/2016/05/31/rl/\n",
    "- https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
